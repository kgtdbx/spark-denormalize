## the resource api version
##
apiVersion: v1

## a data source which is accessed through JDBC
##
kind: JDBCDataSource

## the metadata of this resource
##
metadata:

  ## the name of this data source
  ##
  name: "test" ## TODO: remove after test

## the spec of this resource
##
spec:

  ## the JDBC connection string
  ##
  ## EXAMPLE:
  ##    connection: "jdbc:informix-sqli://..."
  ##
  connection: "test" ## TODO: remove after test

  ## the JDBC driver class to use
  ##
  ## NOTE:
  ##  - JDBC dependencies must be provided with spark-submit configs, like `--jars` or `--packages`
  ##
  ## EXAMPLE:
  ##    driverClass: "com.informix.jdbc.IfxDriver"
  ##
  driverClass: "test" ## TODO: remove after test

  ## configs describing how database credentials are stored
  ##
  ## WARNING:
  ##  - only one option may be specified, the rest must be excluded
  ##
  credentials:

    ## where the credentials are stored in plaintext
    ##
    fromPlaintext:

      ## the plaintext user
      ##
      user: ""

      ## the plaintext password
      ##
      password: ""

    ## where the credentials are stored in an environment variable on the spark master
    ##
    ## NOTE:
    ##  - in spark `--deploy-mode=client`, this is the server running `spark-submit`
    ##
    #fromEnvironment: ## TODO: uncomment after test
    #
    #  ## the environment variable with the plaintext user
    #  ##
    #  ## EXAMPLE:
    #  ##    user: "MY_DB_USER"
    #  ##
    #  userVariable: ""
    #
    #  ## the environment variable with the plaintext password
    #  ##
    #  ## EXAMPLE:
    #  ##    user: "MY_DB_PASSWORD"
    #  ##
    #  passwordVariable: ""

  ## a reference to a table
  ##
  ## NOTE:
  ##  - what "." is interpreted as is dependant on your JDBC driver
  ##
  ## EXAMPLE:
  ##    tableReference: "my_database.my_table"
  ##
  tableReference: "test" ## TODO: remove after test

  ## a list of table predicates (used for read parallelism)
  ##
  ## WARNING:
  ##  - these filters should not overlap, otherwise rows will be duplicated
  ##  - these filters should cover ALL rows in the table, otherwise some rows will be missed
  ##
  ## EXAMPLE -- split on numeric ID column:
  ##    tablePredicates:
  ##      - "mod(my_part_col, 3) = 0"
  ##      - "mod(my_part_col, 3) = 1"
  ##      - "mod(my_part_col, 3) = 2"
  ##      - "mod(my_part_col, 3) = 3"
  ##
  tablePredicates: []

  ## configs describing how snapshots are stored
  ##
  ## WARNING:
  ##  - only one option may be specified, the rest must be excluded
  ##
  snapshot:

    ## where the snapshot time is stored inside a data field
    ##
    fromFieldValue:

      ## the name of the field
      ##
      fieldName: "test" ## TODO: remove after test

      ## the format the snapshot time is stored in
      ##
      ## FORMAT:
      ##  - `ISO8601_DATE`:*       a string with ISO8601 date format (ex: '2020-01-01')
      ##  - `ISO8601_DATE_TIME`:** a string with ISO8601 date-time format (ex: '2020-01-01T00:00:00Z')
      ##  - `TIMESTAMP`:           a timestamp type
      ##  - `UNIX_MILLISECONDS`:   a long with unix epoch in milliseconds
      ##  - `UNIX_SECONDS`:        a long/int with unix epoch in seconds
      ##
      ## *  = read as midnight in the `defaultTimezone`
      ## ** = `defaultTimezone` is used (if missing from string)
      ##
      timeFormat: "UNIX_MILLISECONDS" ## TODO: remove after test

      ## the default timezone ("tz database" format)
      ##
      ## EXAMPLE:
      ##    timezone: "America/New_York"
      ##
      ## NOTE:
      ##  - currently only applicable to: `ISO8601_DATE_TIME`, `ISO8601_DATE`
      ##
      defaultTimezone: "UTC"

      ## if the field is dropped after we extract the snapshot times
      ##
      ## NOTE:
      ##  - any OutputDatasets which use this data source will still have the snapshot of the root data source
      ##
      dropField: true

    ## where the snapshot time is the instant we read the data (system time)
    ##
    ## NOTE:
    ##  - this option makes sense if this data source is the source-of-truth
    ##    (e.g. it is the production system which generates the data)
    ##
    #fromReadTime: {} ## TODO: uncomment after test

  ## manual schema specification
  ##
  ## WARNING:
  ##  - if the schema is incompatible with the data, the data reading may fail
  ##
  ## NOTE:
  ##  - this is OPTIONAL
  ##
  schemaFields:

    - ## the name of the field
      ##
      name: ""

      ## the type of the field
      ##
      ## FORMAT:
      ##  - see `_data_type.yaml`
      ##
      dataType:

        string: {} ## TODO: remove after test

      ## if this field can contain nulls
      ##
      nullable: true

      ## metadata tags to apply to this field
      ##
      metadata: {}

  ## a list of metadata applicators to apply
  ##
  ## NOTE:
  ##  - metadata applicators are applied IN ORDER
  ##
  metadataApplicators:

    - ## the name of a MetadataApplicator resource
      ##
      name: "test" ## TODO: remove after test