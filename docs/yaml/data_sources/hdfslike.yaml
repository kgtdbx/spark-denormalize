## the resource api version
##
apiVersion: v1

## a DataSource which is accessed through an HDFS-like API (e.g. HDFS/S3/GCS)
##
kind: HDFSLikeDataSource

## the metadata of this resource
##
metadata:

  ## the name of this data source
  ##
  name: "test" ## TODO: remove after test

## the spec of this resource
##
spec:

  ## the format of the input data
  ##
  ## NOTE:
  ##  - custom formats can be used if provided with spark-submit configs, like `--jars` or `--packages`
  ##
  ## FORMAT:
  ##  - any value you can put in `DataFrameReader().format()`
  ##
  ## EXAMPLE:
  ##    format: "parquet"
  ##
  format: "test" ## TODO: remove after test

  ## reader options
  ##
  ## NOTE:
  ##  - different values are acceptable depending on your `format`
  ##
  ## FORMAT:
  ##  - a `String -> String` mapping of reader options
  ##  - see spark `DataFrameReader()` docs for valid values:
  ##    https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html
  ##
  ## EXAMPLE -- Excel CSV:
  ##    formatOptions:
  ##      enforceSchema: "false"
  ##      escape: '"'
  ##      header: "true"
  ##      mode: "FAILFAST"
  ##      sep: ","
  ##      multiLine: "true"
  ##
  formatOptions: {}

  ## a list of source URI globs which match the input files
  ##
  ## NOTE:
  ##  - paths to folders will be silently ignored, (to prevent spark recursing into them when reading)
  ##
  ## FORMAT:
  ##  - an HDFS glob pattern:
  ##    https://hadoop.apache.org/docs/r2.8.2/api/org/apache/hadoop/fs/FileSystem.html#globStatus(org.apache.hadoop.fs.Path)
  ##
  ## EXAMPLE -- hive partitioned:
  ##    sourceUriGlobs: ["gs://my_bucket/xxxxx.parquet/snapshot_epoch=*/*.parquet"]
  ##
  ## EXAMPLE -- standard files:
  ##    sourceUriGlobs: ["gs://my_bucket/table_snapshot__*.csv"]
  ##
  sourceUriGlobs: ["test"] ## TODO: remove after test

  ## a URI prefix shared by all matches of `sourceUriGlobs` (used to extract field values encoded into the URI)
  ##
  ## WARNING:
  ##  - if specified, MUST match all matches of `sourceUriGlobs` as a PREFIX, else error
  ##
  ## FORMAT:
  ##  - capture-groups are formatted {<FIELD_NAME>:<DATA_TYPE>}
  ##     - <FIELD_NAME> can be any valid field name (not already present in the source data)
  ##     - <DATA_TYPE> can be one of `STRING`, `INTEGER`, `LONG`, `ISO8601_DATE_TIME`
  ##  - the `{` and `}` characters are escaped with `{{` and `}}` respectively
  ##  - must end with a non capture-group character (e.g. `/`), else error
  ##    (prevents the final capture group consuming the entire remaining URI)
  ##
  ## EXAMPLE -- hive partitioned:
  ##    sourceUriPrefix: "gs://my_bucket/xxxxx.parquet/snapshot_epoch={snapshot_epoch:LONG}/"
  ##
  sourceUriPrefix: "test" ## TODO: remove after test

  ## configs describing how snapshots are stored
  ##
  ## WARNING:
  ##  - only one option may be specified, the rest must be excluded
  ##
  snapshot:

    ## where the snapshot time is stored inside a data field
    ##
    fromFieldValue:

      ## the name of the field
      ##
      ## NOTE:
      ##  - if this field is created by `sourceUriPrefix` extraction, we extract snapshot values based
      ##    off the URI-path directly, (reducing the number of HDFS/S3/GCS API calls)
      ##
      fieldName: "test" ## TODO: remove after test

      ## the format the snapshot time is stored in
      ##
      ## FORMAT:
      ##  - `ISO8601_DATE`:*       a string with ISO8601 date format (ex: '2020-01-01')
      ##  - `ISO8601_DATE_TIME`:** a string with ISO8601 date-time format (ex: '2020-01-01T00:00:00Z')
      ##  - `TIMESTAMP`:           a timestamp type
      ##  - `UNIX_MILLISECONDS`:   a long with unix epoch in milliseconds
      ##  - `UNIX_SECONDS`:        a long/int with unix epoch in seconds
      ##
      ## *  = read as midnight in the `defaultTimezone`
      ## ** = `defaultTimezone` is used (if missing from string)
      ##
      timeFormat: "UNIX_MILLISECONDS" ## TODO: remove after test

      ## the default timezone ("tz database" format)
      ##
      ## EXAMPLE:
      ##    timezone: "America/New_York"
      ##
      ## NOTE:
      ##  - currently only applicable to: `ISO8601_DATE_TIME`, `ISO8601_DATE`
      ##
      defaultTimezone: "UTC"

      ## if the field is dropped after we extract the snapshot times
      ##
      ## WARNING:
      ##  - if false, `fieldName` must never appear in the data,
      ##    or the job will fail at write-time due to field name collision
      ##
      dropField: true

    ## where the snapshot time is the instant we read the data (system time)
    ##
    ## NOTE:
    ##  - this option makes sense if this data source is the source-of-truth
    ##    (e.g. it is the production system which generates the data)
    ##
    #fromReadTime: {} ## TODO: uncomment after test

  ## manual schema specification
  ##
  ## WARNING:
  ##  - if the schema is incompatible with the data, the data reading may fail
  ##
  ## NOTE:
  ##  - this is OPTIONAL if the input format supports schema detection
  ##
  schemaFields:

    - ## the name of the field
      ##
      name: ""

      ## the type of the field
      ##
      ## FORMAT:
      ##  - see `_data_type.yaml`
      ##
      dataType:

        string: {} ## TODO: remove after test

      ## if this field can contain nulls
      ##
      nullable: true

      ## metadata tags to apply to this field
      ##
      metadata: {}

  ## a list of metadata applicators to apply
  ##
  ## NOTE:
  ##  - metadata applicators are applied IN ORDER
  ##
  metadataApplicators:

    - ## the name of a MetadataApplicator resource
      ##
      name: "test" ## TODO: remove after test